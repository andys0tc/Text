{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep N-grams\n",
    "\n",
    "### Predecir el siguiente conjunto de caracteres usando los caractees previos\n",
    "\n",
    "### Tareas por realizar\n",
    "1. Convertir texto en tensores\n",
    "2. Crear iterador para alimentar los datos al modelo\n",
    "3. Definir modelo GRU con trax\n",
    "4. Entrenar modelo con trax\n",
    "5. Calcular la precisión del modelo usando perplexity\n",
    "6. Hacer predicciones con el modelo generado\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import trax\n",
    "import trax.fastmath.numpy as np\n",
    "import pickle\n",
    "import numpy\n",
    "import random as rnd\n",
    "from trax import fastmath\n",
    "from trax import layers as tl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "dirname = '/media/andrea/Baba Yaga/BIGFOOT/CIC/PycharmProjects/PycharmProjects/semester work/Proy4/data/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cargando datos\n",
    "\n",
    "Como texto usaremos algunas obras de Shakespeare\n",
    "\n",
    "### Preprocesamiento de los datos\n",
    "* Para la generacion de caracteres, cara caracter debe tener un número único\n",
    "* Todos los caracteres se transforman a minpusculas\n",
    "* Se usa la funcion $ord$ para convertir a un único entero un caracter\n",
    "* Crear un generador que regresa batches del conjunto de datos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#dirname = 'data/'\n",
    "lines = [] # storing all the lines in a variable.\n",
    "for filename in os.listdir(dirname):\n",
    "    with open(os.path.join(dirname, filename)) as files:\n",
    "        for line in files:\n",
    "            pure_line = line.strip()\n",
    "\n",
    "            if pure_line:\n",
    "                lines.append(pure_line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "n_lines = len(lines)\n",
    "print(f\"Number of lines: {n_lines}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 124097\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Todo e texto se transformará en minpuscualas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "for i, line in enumerate(lines):\n",
    "    lines[i] = line.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number for training: 123097\n",
      "Number for validation: 1000\n"
     ]
    }
   ],
   "source": [
    "eval_lines = lines[-1000:] # Create a holdout validation set\n",
    "lines = lines[:-1000] # Leave the rest for training\n",
    "\n",
    "\n",
    "print(f\"Number for training: {len(lines)}\")\n",
    "print(f\"Number for validation: {len(eval_lines)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Línea a tensor\n",
    "Toma como entrada una línea y transforma cada caracter a s forma unicode entera, y regresa lista de enteros(tensor)\n",
    "Agregar al final de la oración el caracter especial"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def line_to_tensor(line, EOS_int=1):\n",
    "    tensor = []\n",
    "    for c in line:\n",
    "        c_int = ord(c)\n",
    "        tensor.append(c_int)\n",
    "    tensor.append(EOS_int)\n",
    "    return tensor\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "[97, 98, 99, 32, 120, 121, 122, 1]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_to_tensor('abc xyz')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Batch genetaror\n",
    "\n",
    "Generador por bloques de texto para entrenamiento, validación y pruebas.\n",
    "* El generador convierte las líneas de texto en arreglos de numpy \"rellenos\" de ceros para que todos tengan la misma longitud\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor, shuffle=True):\n",
    "    index = 0\n",
    "    cur_batch = []\n",
    "    num_lines = len(data_lines)\n",
    "    lines_index = [*range(num_lines)]\n",
    "    if shuffle:\n",
    "        rnd.shuffle(lines_index)\n",
    "    while True:\n",
    "        if index>=num_lines:\n",
    "            index = 0\n",
    "            if shuffle:\n",
    "                rnd.shuffle(lines_index)\n",
    "        line = data_lines[index]\n",
    "\n",
    "        if len(line)<max_length:\n",
    "            cur_batch.append(line)\n",
    "        index += 1\n",
    "\n",
    "        if len(cur_batch)==batch_size:\n",
    "            batch = []\n",
    "            mask = []\n",
    "            for li in cur_batch:\n",
    "                tensor = line_to_tensor(li)\n",
    "                pad = [0] * (max_length-len(tensor))\n",
    "                tensor_pad = tensor+pad\n",
    "                batch.append(tensor_pad)\n",
    "                example_mask = [0 if i==0 else 1  for i in tensor_pad]\n",
    "                mask.append(example_mask)\n",
    "            batch_np_arr = np.array(batch)\n",
    "\n",
    "            mask_np_arr = np.array(mask)\n",
    "\n",
    "            yield batch_np_arr, batch_np_arr, mask_np_arr\n",
    "\n",
    "            cur_batch = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Probando generador"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tmp_lines = ['12345678901', #11\n",
    "             '123456789', # 9\n",
    "             '234567890', # 9\n",
    "             '345678901'] # 9\n",
    "\n",
    "# Get a batch size of 2, max length 10\n",
    "tmp_data_gen = data_generator(batch_size=2,\n",
    "                              max_length=10,\n",
    "                              data_lines=tmp_lines,\n",
    "                              shuffle=False)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_data_gen)\n",
    "\n",
    "# view the batch\n",
    "tmp_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import torch, jax; print(torch.cuda.is_available()); print(jax.devices())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Repitiendo generador de batches\n",
    "La función ```itertools.cycle``` es util para que el generador eventualmente se detenga"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "infinite_data_generator = itertools.cycle(\n",
    "    data_generator(batch_size=2, max_length=10, data_lines=tmp_lines))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "ten_lines = [next(infinite_data_generator) for _ in range(10)]\n",
    "print(len(ten_lines))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model with GRU\n",
    "Implementando modelo GRU (Gated Recurrent Unit).\n",
    "Para la construcción del modelo con trax es necesario los siguientes paquetes:\n",
    "* ```tl.Serial```: Permite aplicar capas sucesivas\n",
    "* ```tl.ShiftRight```: Permite que el modelo pase hacia adelante(feed forward)\n",
    "* ```tl.Embedding```: Inicializa el embedding del tamaño del vocabulario y dimensión del modelo\n",
    "* ```tl.GRU```: Construye un GRU con n_cells unidades\n",
    "* ```tl.Dense```: N_unidades de salida para la capa densa\n",
    "* ```tl.LogSoftmax```: Log de la probabilidad de la salida"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
    "\n",
    "    model = tl.Serial(\n",
    "      tl.ShiftRight(mode=mode), # Stack the ShiftRight layer\n",
    "      tl.Embedding(vocab_size = vocab_size,d_feature=d_model), # Stack the embedding layer\n",
    "      [tl.GRU(n_units=d_model) for _ in range(n_layers)], # Stack GRU layers of d_model units keeping n_layer parameter in mind (use list comprehension syntax)\n",
    "      tl.Dense(n_units=vocab_size), # Dense layer\n",
    "      tl.LogSoftmax() # Log Softmax\n",
    "    )\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Serial[\n",
      "    ShiftRight(1)\n",
      "  ]\n",
      "  Embedding_256_512\n",
      "  GRU_512\n",
      "  GRU_512\n",
      "  Dense_256\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = GRULM()\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_length = 64"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of used lines from the dataset: 25773\n",
      "Batch size (a power of 2): 32\n",
      "Number of steps to cover one epoch: 805\n"
     ]
    }
   ],
   "source": [
    "def n_used_lines(lines, max_length):\n",
    "\n",
    "\n",
    "    n_lines = 0\n",
    "    for l in lines:\n",
    "        if len(l) <= max_length:\n",
    "            n_lines += 1\n",
    "    return n_lines\n",
    "\n",
    "num_used_lines = n_used_lines(lines, 32)\n",
    "print('Number of used lines from the dataset:', num_used_lines)\n",
    "print('Batch size (a power of 2):', int(batch_size))\n",
    "steps_per_epoch = int(num_used_lines/batch_size)\n",
    "print('Number of steps to cover one epoch:', steps_per_epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "output_dir = '/media/andrea/Baba Yaga/BIGFOOT/CIC/PycharmProjects/PycharmProjects/semester work/Proy4/model/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "def train_model(model, data_generator, batch_size=32, max_length=64, lines=lines, eval_lines=eval_lines, n_steps=100,output_dir=output_dir):\n",
    "    print(\"Dir\")\n",
    "    print(output_dir)\n",
    "    bare_train_generator = data_generator(batch_size=batch_size, max_length=max_length, data_lines=lines)\n",
    "    infinite_train_generator =  itertools.cycle(bare_train_generator)\n",
    "\n",
    "    bare_eval_generator = data_generator(batch_size=batch_size, max_length=max_length, data_lines=eval_lines)\n",
    "    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
    "\n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=infinite_train_generator,\n",
    "        loss_layer= tl.CrossEntropyLoss(),\n",
    "        optimizer=trax.optimizers.Adam(learning_rate=0.0005)\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=infinite_eval_generator,\n",
    "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    "        n_eval_batches=3\n",
    "    )\n",
    "\n",
    "    training_loop = training.Loop(model,\n",
    "                                  train_task,\n",
    "                                  eval_tasks=[eval_task],\n",
    "                                  output_dir=output_dir)\n",
    "\n",
    "    training_loop.run(n_steps=n_steps)\n",
    "\n",
    "    return training_loop\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir\n",
      "/media/andrea/Baba Yaga/BIGFOOT/CIC/PycharmProjects/PycharmProjects/semester work/Proy4/model/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/pytorch/lib/python3.8/site-packages/jax/lib/xla_bridge.py:355: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n",
      "/home/andrea/miniconda3/envs/pytorch/lib/python3.8/site-packages/jax/lib/xla_bridge.py:368: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step    100: Ran 99 train steps in 62.02 secs\n",
      "Step    100: train CrossEntropyLoss |  3.35154223\n",
      "Step    100: eval  CrossEntropyLoss |  2.88437454\n",
      "Step    100: eval          Accuracy |  0.19368572\n"
     ]
    }
   ],
   "source": [
    "training_loop = train_model(GRULM(), data_generator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluación\n",
    "\n",
    "Usaremos preplejidad para evaluar que tan bien lo ha hecho el modelo\n",
    "Perplexity definida como:\n",
    "$$P(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}$$\n",
    "\n",
    "$$log P(W) = {log\\big(\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)}$$\n",
    "\n",
    "$$ = {log\\big({\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)^{\\frac{1}{N}}}$$\n",
    "\n",
    "$$ = {log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)^{-\\frac{1}{N}}} $$\n",
    "$$ = -\\frac{1}{N}{log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)} $$\n",
    "$$ = -\\frac{1}{N}{\\big({\\sum_{i=1}^{N}{logP(w_i| w_1,...,w_{n-1})}}\\big)} $$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def test_model(preds, target):\n",
    "\n",
    "    total_log_ppx = np.sum(tl.one_hot(target,preds.shape[-1]) * preds, axis= -1) # HINT: tl.one_hot() should replace one of the Nones\n",
    "\n",
    "    non_pad = 1.0 - np.equal(target, 0)\n",
    "    ppx = total_log_ppx * non_pad\n",
    "\n",
    "    log_ppx = np.sum(ppx) / np.sum(non_pad)\n",
    "\n",
    "    return -log_ppx\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log perplexity and perplexity of your model are respectively 4.498601 89.89128\n",
      "The log perplexity and perplexity of your model are respectively 4.498601 89.89128\n"
     ]
    }
   ],
   "source": [
    "model = GRULM()\n",
    "model.init_from_file('/media/andrea/Baba Yaga/BIGFOOT/CIC/PycharmProjects/PycharmProjects/semester work/Proy4/model.pkl.gz')\n",
    "batch = next(data_generator(batch_size, max_length, lines, shuffle=False))\n",
    "preds = model(batch[0])\n",
    "log_ppx = test_model(preds, batch[1])\n",
    "print('The log perplexity and perplexity of your model are respectively', log_ppx, np.exp(log_ppx))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generando lenguaje con nuestro modelo\n",
    "Para este modelo haremos muestreo de la distribución Gumbel, de esta forma podemos generar nuevas oraciones.\n",
    "La Función de Densidad de Probabilidad de Gumbel es definida como:\n",
    "$$ f(z) = {1\\over{\\beta}}e^{(-z+e^{(-z)})} $$\n",
    "\n",
    "where: $$ z = {(x - \\mu)\\over{\\beta}}$$\n",
    "\n",
    "Cuando una variable aleatoria tiene un crecimiento exponencial, la distribución Gumbel se acerca cuando el muestreo crece asintóticamente."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gumbel_sample(log_probs, temperature=1.0):\n",
    "    \"\"\"Gumbel sampling from a categorical distribution.\"\"\"\n",
    "    u = numpy.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
    "    g = -np.log(-np.log(u))\n",
    "    return np.argmax(log_probs + g * temperature, axis=-1)\n",
    "\n",
    "def predict(num_chars, prefix):\n",
    "    inp = [ord(c) for c in prefix]\n",
    "    result = [c for c in prefix]\n",
    "    max_len = len(prefix) + num_chars\n",
    "    for _ in range(num_chars):\n",
    "        cur_inp = np.array(inp + [0] * (max_len - len(inp)))\n",
    "        outp = model(cur_inp[None, :])  # Add batch dim.\n",
    "        next_char = gumbel_sample(outp[0, len(inp)])\n",
    "        inp += [int(next_char)]\n",
    "\n",
    "        if inp[-1] == 1:\n",
    "            break  # EOS\n",
    "        result.append(chr(int(next_char)))\n",
    "\n",
    "    return \"\".join(result)\n",
    "\n",
    "print(predict(32, \"\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Los siguentes son textos generados por el modelo y captura las dependencias entre las palabras sin necesidad de alguna entrada."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAY\tNot worth the fools.\n",
      "But let the instrument\n",
      "SIMPLE\tWhich are not membrayetly\n"
     ]
    }
   ],
   "source": [
    "print(predict(32, \"\"))\n",
    "print(predict(32, \"\"))\n",
    "print(predict(32, \"\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love him once married, run, sir; our gates and places \n"
     ]
    }
   ],
   "source": [
    "print(predict(50,\"love\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}